# LLM_info
Useful information about LLM and its environment is collected here

# Open source projects and frameworks
1. [vLLM](https://github.com/vllm-project/vllm): a fast and easy-to-use library for LLM inference and serving ([blog](https://blog.vllm.ai/2023/06/20/vllm.html))
2. [DeepSpeed](https://github.com/microsoft/DeepSpeed): a deep learning optimization library that makes distributed training easy, efficient, and effective
3. [TVM](https://github.com/apache/tvm): a compiler stack for deep learning systems. It is designed to close the gap between the productivity-focused deep learning frameworks, and the performance- and efficiency-focused hardware backends
4. [LangChain](https://github.com/langchain-ai/langchain) is a framework for developing applications powered by large language models (LLMs)
5. [FireOptimizer](https://fireworks.ai/blog/fireoptimizer?utm_source=newsletter&utm_medium=email&utm_campaign=2024september): customizing latency and quality for your production inference workload
6. [GGML](https://github.com/ggerganov/ggml): tensor library for machine learning
7. [Medusa](https://github.com/FasterDecoding/Medusa) is a simple framework that democratizes the acceleration techniques for LLM generation with multiple decoding heads.
8. [Optimal Brain Compression (OBC)](https://github.com/IST-DASLab/OBC): a framework for accurate PTQ and pruning ([paper](https://github.com/vvchernov/LLM_info/blob/main/papers/common/OBC.pdf))

# AI Agents and platforms
1. [OPEA](https://www.intel.com/content/www/us/en/developer/articles/news/genai-project-opea-marks-1-0-release.html): Open Platform for Enterprise AI from Intel ([examples](https://github.com/opea-project/GenAIExamples) in github)
 - [GenAIComps](https://github.com/opea-project/GenAIComps): a service-based tool that includes microservice components such as llm, embedding, reranking, and so on
 - [GenAIInfra](https://github.com/opea-project/GenAIInfra): part of the OPEA containerization and cloud-native suite, enables quick and efficient deployment of GenAIExamples in the cloud
 - [GenAIEval](https://github.com/opea-project/GenAIEval): it measures service performance metrics such as throughput, latency, and accuracy for GenAIExamples. This feature helps users compare performance across various hardware configurations easily.

# Common information in tutorials and blogs

## Optimization concepts
1. Speculative decoding:
 - [Speculative Decoding â€” Make LLM Inference Faster](https://medium.com/ai-science/speculative-decoding-make-llm-inference-faster-c004501af120)

## API
1. [OpenAI API](https://platform.openai.com/docs/api-reference/introduction)
2. [Using logprobs](https://cookbook.openai.com/examples/using_logprobs) from OpenAI

## Benchmarks
1. [LLM evals and benchmarking](https://osanseviero.github.io/hackerllama/blog/posts/llm_evals/)

# Papers
[Raw list](https://github.com/vvchernov/LLM_info/blob/main/papers/README.md) of papers sorted by general topics<br />
Brief description and analysis of current state based on the papers can be also found there.
