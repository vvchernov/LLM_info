# Compression
1. [Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems]()

## Quantization

### Before LLM era (classic)
1. [Quantization and training of neural networks for efficient integer-arithmetic-only inference](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/before_llm/NN_with_int_arithmetic_only.pdf)
2. [Pointer Sentinel Mixture Models](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/before_llm/sentinel_mixture_models.pdf)
3. [MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/before_llm/MixMix.pdf)
4. [Low-bit Quantization of Neural Networks for Efficient Inference](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/before_llm/low_bit_quant.pdf)
5. [Quantization Networks](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/before_llm/quantization_networks.pdf) ([github](https://github.com/aliyun/alibabacloud-quantization-networks))
6. [Compressing Large-Scale Transformer-Based Models: A Case Study on BERT](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/before_llm/compressing_large_scale_transformer_based_models.pdf)
7. [APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores]() ([github](https://github.com/BoyuanFeng/APNN-TC))
8. [Towards Accurate Post-training Network Quantization via Bit-Split and Stitching]()

### 2021
1. [BRECQ: pushing the limit of post-training quantization by block reconstruction](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/BRECQ.pdf) ([github](https://github.com/yhhhli/BRECQ))
2. [HAWQ-V3: dyadic neural network quantization](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/HAWQ-v3.pdf) ([github](https://github.com/zhen-dong/hawq.git))

### 2022
1. [ZeroQuant: efficient and affordable post-training quantization for large-scale transformers](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/ZeroQuant.pdf) (implemented in [DeepSpeed](https://github.com/microsoft/DeepSpeed))
2. [NuQMM: Quantized MatMul for efficient inference of large-scale generative language models]()

### 2023
1. [SmoothQuant: accurate and efficient post-training quantization for large language models](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/SmoothQuant.pdf) ([github](https://github.com/mit-han-lab/smoothquant))
2. [AWQ: Activation-aware Weight Quantization for LLM compression and acceleration](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/AWQ.pdf) ([github](https://github.com/mit-han-lab/llm-awq))
3. [Outlier suppression: pushing the limit of low-bit transformer language models](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/OutlierSuppression.pdf) ([github](https://github.com/wimh966/outlier_suppression))
4. [Outlier suppression+: accurate quantization of large language models by equivalent and effective shifting and scaling](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/OutlierSuppresion_plus.pdf) ([github](https://github.com/ModelTC/Outlier_Suppression_Plus))
5. [GPTQ: accurate post-training quantization for generative pre-trained transformers](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/GPTQ.pdf) ([github](https://github.com/IST-DASLab/gptq))
6. [SpQR: A Sparse-Quantized Representation for near-lossless LLM weight compression](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/SpQR.pdf)
7. [The case for 4-bit precision: k-bit inference scaling laws](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/kbit-inference-scaling-laws.pdf)
8. [Enhancing computation efficiency in large language models through weight and activation quantization](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/aqas_slac.pdf)
9. [MixQuant: A Quntization bit-width search that can optimize the performance of your quantization method](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/MixQuant.pdf)

### 2024
1. [Efficient post-training quantization with fp8 formats](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/fp8_ptq.pdf)
2. [OmniQuant: Omnidirectionally calibrated quantization for large language models](https://github.com/vvchernov/LLM_info/blob/main/papers/compression/quantization/omni_quant.pdf) ([github](https://github.com/OpenGVLab/OmniQuant))
